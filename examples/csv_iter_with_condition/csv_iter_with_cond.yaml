apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: scargo-csv-iter-cond-
spec:
  volumes:
    - name: workdir
      emptyDir: {}

  arguments:
    parameters:
      - name: s3-bucket
      - name: input-path
      - name: output-path
      - name: input-csv
      - name: optimized

  entrypoint: main

  templates:
    - name: main
      steps:
        - - name: get-inputs
            # Produces the complete set of work units from the initial input
            template: split-csv
            arguments:
              artifacts:
                - name: csv-file
                  s3:
                    endpoint: s3.amazonaws.com
                    bucket: "{{workflow.parameters.s3-bucket}}"
                    key: "{{workflow.parameters.input-path}}/{{workflow.parameters.input-csv}}"

        - - name: process-each
            # Iterates over the a set of work units produced by the previous step
            template: compute-one
            arguments:
              parameters:
                - name: index
                  value: "{{item}}"
              artifacts:
                - name: mappings
                  from: "{{steps.get-inputs.outputs.artifacts.json-data}}"
            withSequence:
              count: "{{steps.get-inputs.outputs.parameters.length}}"

    - name: compute-one
      # Processes a single work unit
      inputs:
        parameters:
          - name: index
        artifacts:
          - name: mappings
      steps:
        - - name: get-work-item
            # Retrieves the artifact references (PDB name, sequence) that are required to process a single unit of work
            template: get-work-item
            arguments:
              parameters:
                - name: index
                  value: "{{inputs.parameters.index}}"
              artifacts:
                - name: mappings
                  from: "{{inputs.artifacts.mappings}}"

        - - name: add-one-compute
            template: exec-add-one
            arguments:
              parameters:
                - name: init-value
                  value: "{{steps.get-work-item.outputs.parameters.command-arg}}"
            when: "{{steps.get-work-item.outputs.parameters.command-type}} == add_one"
          - name: add-two-compute
            template: exec-add-two
            arguments:
              parameters:
                - name: init-value
                  value: "{{steps.get-work-item.outputs.parameters.command-arg}}"
            when: "{{steps.get-work-item.outputs.parameters.command-type}} == add_two"

    - name: get-work-item
      # From a given JSON array, get the item at `index`, which is expected to be an object,
      # and output the values of its 'filename' and 'sequence' keys
      inputs:
        parameters:
          - name: index
        artifacts:
          - name: mappings
            path: /tmp/mappings.json
      outputs:
        parameters:
          - name: command-type
            valueFrom:
              path: /tmp/command-type
          - name: command-arg
            valueFrom:
              path: /tmp/command-arg
      script:
        image: stedolan/jq
        command: [sh]
        source: |
          jq -r '.[{{inputs.parameters.index}}].commandtype' {{inputs.artifacts.mappings.path}} > {{outputs.parameters.command-type.path}}
          jq -r '.[{{inputs.parameters.index}}].commandarg' {{inputs.artifacts.mappings.path}} > {{outputs.parameters.command-arg.path}}
        resources:
          requests:
            memory: 30Mi
            cpu: 20m
          limits:
            memory: 30Mi
            cpu: 20m

    - name: split-csv
      # Given a CSV file, convert each row into a JSON-formatted object,
      # and output the list all resulting objects as an artifact,
      # and the length of this list as a parameter
      inputs:
        artifacts:
          - name: csv-file
            path: /tmp/input.csv
      script:
        image: python:alpine
        command: [python]
        source: |
          from csv import reader
          import json

          with open("{{inputs.artifacts.csv-file.path}}", "r") as f:
            rows = reader(f)
            next(rows)
            data = [ {"commandtype": r[0], "commandarg": r[1]} for r in list(rows) ]

          with open("{{outputs.artifacts.json-data.path}}", "w") as f:
            f.write(json.dumps(data))

          with open("{{outputs.parameters.length.path}}", "w") as f:
            f.write(str(len(data)))
        resources:
          requests:
            memory: 30Mi
            cpu: 20m
          limits:
            memory: 30Mi
            cpu: 20m
      outputs:
        parameters:
          - name: length
            valueFrom:
              path: /tmp/length
        artifacts:
          - name: json-data
            path: /tmp/data.json

    - name: exec-add-one
      inputs:
        parameters:
          - name: init-value
      outputs:
        artifacts:
          - name: txt-out
            path: /workdir/out
            archive:
              none: {}
            s3:
              endpoint: s3.amazonaws.com
              bucket: "{{workflow.parameters.s3-bucket}}"
              key: "{{workflow.parameters.output-path}}"
      initContainers:
        - name: mkdir
          image: alpine:latest
          command: ["mkdir", "-p", "/workdir/out"]
          mirrorVolumeMounts: true
        - name: chmod
          image: alpine:latest
          command: ["chmod", "-R", "a+rwX", "/workdir"]
          mirrorVolumeMounts: true
      script:
        image: python:alpine
        command: [python]
        source: |
          with open("{{outputs.artifacts.txt-out.path}}/add_one_{{inputs.parameters.init-value}}.txt", "w+") as fi:
              fi.write(str({{inputs.parameters.init-value}} + 1))

        resources:
          requests:
            memory: 30Mi
            cpu: 20m
          limits:
            memory: 30Mi
            cpu: 20m
        volumeMounts:
          - name: workdir
            mountPath: /workdir

    - name: exec-add-two
      inputs:
        parameters:
          - name: init-value
      outputs:
        artifacts:
          - name: txt-out
            path: /workdir/out
            archive:
              none: {}
            s3:
              endpoint: s3.amazonaws.com
              bucket: "{{workflow.parameters.s3-bucket}}"
              key: "{{workflow.parameters.output-path}}"
      initContainers:
        - name: mkdir
          image: alpine:latest
          command: ["mkdir", "-p", "/workdir/out"]
          mirrorVolumeMounts: true
        - name: chmod
          image: alpine:latest
          command: ["chmod", "-R", "a+rwX", "/workdir"]
          mirrorVolumeMounts: true
      script:
        image: python:alpine
        command: [python]
        source: |
          result = str({{inputs.parameters.init-value}} + 2)

          with open("{{outputs.artifacts.txt-out.path}}/add_two_{{inputs.parameters.init-value}}.txt", "w+") as fi:
              fi.write(result)

        resources:
          requests:
            memory: 30Mi
            cpu: 20m
          limits:
            memory: 30Mi
            cpu: 20m
        volumeMounts:
          - name: workdir
            mountPath: /workdir
