apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: scargo-bash-step-
spec:
  volumes:
    - name: workdir
      emptyDir: {}

  arguments:
    parameters:
      - name: project
      - name: input-s3-bucket
      - name: input-s3-path
      - name: input-csv
      - name: output-s3-bucket
      - name: output-s3-path
      - name: optimized

  entrypoint: main

  templates:
    - name: main
      steps:
        - - name: get-first-col
            template: exec-get-first-col
            arguments:
              artifacts:
                - name: csv-file
                  s3:
                    endpoint: s3.amazonaws.com
                    bucket: "{{workflow.parameters.input-s3-bucket}}"
                    key: "{{workflow.parameters.input-s3-path}}/{{workflow.parameters.input-csv}}"

    - name: exec-get-first-col
      inputs:
        artifacts:
          - name: csv-file
            path: /workdir/in/input.csv
      outputs:
        artifacts:
          - name: txt-out
            path: /workdir/out
            archive:
              none: {}
            s3:
              endpoint: s3.amazonaws.com
              bucket: "{{workflow.parameters.output-s3-bucket}}"
              key: "{{workflow.parameters.output-s3-path}}"
      initContainers:
        - name: mkdir
          image: alpine:latest
          command: ["mkdir", "-p", "/workdir/out", "/workdir/in"]
          mirrorVolumeMounts: true
        - name: chmod
          image: alpine:latest
          command: ["chmod", "-R", "a+rwX", "/workdir"]
          mirrorVolumeMounts: true
      script:
        image: fedora:33
        command: [bash]
        source: |
          set -xe

          cat {{inputs.artifacts.csv-file.path}} | cut -d ',' -f 1 > {{outputs.artifacts.txt-out.path}}/command-types.txt

        resources:
          requests:
            memory: 30Mi
            cpu: 20m
          limits:
            memory: 30Mi
            cpu: 20m
        volumeMounts:
          - name: workdir
            mountPath: /workdir
