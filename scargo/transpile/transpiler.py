"""
Core functionality of the Python -> Argo YAML transpiler.
"""

import ast
from pathlib import Path
from typing import Dict, Union
import yaml

import astpretty

from scargo.core import WorkflowParams
from scargo.errors import ScargoTranspilerError
from scargo.transpile.utils import hyphenate, Transput


class ScargoTranspiler(ast.NodeVisitor):
    """
    Extracts and transpiles the scargo Python script to an Argo YAML workflow
    file.
    """

    def __init__(self):
        """
        Configures the Argo headers.
        """

        # initialize the transpiled workflow dictionary
        # with the typical Argo header
        self.transpiled_workflow = {
            "apiVersion": "argoproj.io/v1alpha1",
            "kind": "Workflow",
            "metadata": {"generateName": None},  # will be defined in self.transpile()
            "spec": {
                "volumes": {"name": "workdir", "emptyDir": {}},
            },
        }

    @staticmethod
    def _get_script_locals(path_to_script: Path) -> Dict:
        """
        Execute the script (without actually running the __main__ function) in
        order to get convenient access to the locals() generated by the script.
        """

        script_locals = {}
        with open(path_to_script, "r") as script:
            exec(script.read(), {}, script_locals)  # no need to keep the globals, everything we need is in the locals

        return script_locals

    def transpile_parameters(self, path_to_script: Path) -> None:
        """
        Retrieves the global workflow parameters from the scargo Python script
        and writes them to YAML.
        """
        script_locals = self._get_script_locals(path_to_script)
        workflow_param_variables = [value for value in script_locals.values() if isinstance(value, WorkflowParams)]

        if not workflow_param_variables:
            raise ScargoTranspilerError("No globally defined WorkflowParams object found.")
        elif len(workflow_param_variables) > 1:
            raise ScargoTranspilerError("Multiple global WorkflowParams objects found. Please only define one.")

        self._write_params_to_yaml(path_to_script, workflow_param_variables[0])
        return workflow_param_variables[0]

    def transpile(self, path_to_script: Path) -> None:
        """
        Convert the script to AST, traverse the tree and transpile the Python
        statements to an Argo workflow.
        """
        self.script_locals = self._get_script_locals(path_to_script)
        workflow_params = self.transpile_parameters(path_to_script)

        # set the Argo workflow name based on the script name
        hyphenated_script_name = path_to_script.stem.replace("_", "-")
        self.transpiled_workflow["metadata"]["generateName"] = f"scargo-{hyphenated_script_name}-"

        # transpile the parameters and write them to a separate YAML
        # as well as include their names in the main YAML workflow file
        self.transpiled_workflow["arguments"] = {"parameters": [{"name": name} for name in workflow_params]}

        # parse the AST tree
        with open(path_to_script, "r") as source:
            self.tree = ast.parse(source.read())
        self.visit(self.tree)  # traverse the tree

        # add entrypoint
        self.transpiled_workflow["entrypoint"] = self.entrypoint

        # TODO add step templates
        templates = []
        entrypoint_template = {
            "name": self.entrypoint,
            "steps": [
                {
                    "name": hyphenate(step.name),
                    "template": f"{hyphenate(step.name)}-template",
                    "arguments": {
                        "parameters": [
                            {
                                "name": name,
                                "value": value,
                            }
                            for name, value in step.inputs.parameters.items()
                        ]
                    },
                }
                for step in self.steps
            ],
        }
        templates.append(entrypoint_template)

        # TODO: add templates
        self.transpiled_workflow["templates"] = templates

        self._write_workflow_to_yaml(path_to_script, self.transpiled_workflow)

    def visit_FunctionDef(self, node: ast.FunctionDef) -> None:
        """
        Visits all FunctionDef nodes and retrieves:
            - the name of the function with the @entrypoint decorator
        """

        if isinstance(node.decorator_list[0], ast.Name):
            if node.decorator_list[0].id == "entrypoint":
                self.entrypoint = node.name

                self.steps = []
                for expression in node.body:
                    if isinstance(expression.value, ast.Call):
                        # TODO: maybe create simple Steps class that parses
                        # this into an easily accessible object?
                        self.steps.append(
                            WorkflowStep(call_node=expression.value, locals_context=self.script_locals, tree=self.tree)
                        )
                        pass

    @staticmethod
    def _write_workflow_to_yaml(path_to_script: Path, transpiled_workflow: Dict) -> None:
        """
        Writes the `transpiled_workflow` to a YAML file in the same directory as the
        original Python input script.
        """

        filename = f"{path_to_script.stem.replace('_', '-')}.yaml"
        with open(path_to_script.parent / filename, "w+") as yaml_out:
            yaml.dump(transpiled_workflow, yaml_out, sort_keys=False)

    @staticmethod
    def _write_params_to_yaml(path_to_script: Path, parameters: Dict) -> None:
        """
        Writes the `parameters` to a YAML file in the same directory as the
        original Python input script.
        """

        filename = f"{path_to_script.stem.replace('_', '-')}-parameters.yaml"
        with open(path_to_script.parent / filename, "w+") as yaml_out:
            yaml.dump(dict(parameters), yaml_out)


class WorkflowStep:
    """
    A single step/template in the workflow.
    """

    def __init__(self, call_node: ast.Call, locals_context: Dict, tree: ast.Module) -> None:
        """
        Create a new WorkflowStep.

        Parameters
        ----------
        call_node : ast.Call
            The Call node that invokes this workflow step. In other words the
            Call node which calls the @scargo decorated function in the
            @entrypoint function.
        locals_context : Dict
            The locals() context resulting from `exec`uting the Python scargo
            script. Should contain all the global imports, variables and
            function definitions.
        tree : ast.Module
            The entire (!) Abstract Syntax Tree of the Python scargo script.
        """
        self.call_node = call_node
        self.locals_context = locals_context
        self.tree = tree

    @property
    def name(self) -> str:
        """
        Returns the name of the workflow step.
        """
        return self.call_node.func.id

    @staticmethod
    def _get_variable_from_args_or_kwargs(
        node: ast.Call, variable_name: str, arg_index: int
    ) -> Union[ast.keyword, ast.Dict]:
        """
        We don't know if the user has passed a positional or a keyworded
        argument to `node` which result in different ASTs. Since this is a
        common occurence, this method figures it out for you and returns the
        node representing the variable.
        """
        if node.args:
            variable_node = node.args[arg_index]
        elif node.keywords:
            variable_node = list(filter(lambda k: k.arg == variable_name, node.keywords))[0].value
        else:
            raise ScargoTranspilerError(f"Can't parse {variable_name} from {node.func.id}.")

        return variable_node

    def _resolve_parameter_value(self, raw_value: ast.Subscript) -> str:
        """
        Given a Subscript node (`raw_value`) this method uses the
        locals_context of the scargo script and the AST to resolve this node
        either into the actual parameter value or into a reference to the
        global Argo workflow parameters.
        """

        if isinstance(raw_value, ast.Subscript):
            # could be a list, tuple or dict
            subscripted_object = raw_value.value.id
            if subscripted_object in self.locals_context:
                if isinstance(self.locals_context[subscripted_object], WorkflowParams):
                    value = "{{" + f"workflow.parameters.{raw_value.slice.value.value}" + "}}"

        return value

    @property
    def inputs(self) -> Transput:
        """
        Parses the input parameters and artifacts from the workflow step and
        returns them as a Transput object (which has a `parameters` and an
        `artifacts` attribute for easy access.
        """
        scargo_inputs = self._get_variable_from_args_or_kwargs(self.call_node, "scargo_in", 0)
        if scargo_inputs.func.id != "ScargoInput":
            raise ScargoTranspilerError("First argument to a @scargo function must be a `ScargoInput`.")

        raw_parameters = self._get_variable_from_args_or_kwargs(scargo_inputs, "parameters", 0)
        parameters = {}
        for key, value in zip(raw_parameters.keys, raw_parameters.values):
            parameters[key.value] = self._resolve_parameter_value(value)

        # TODO: need to process input artifacts

        return Transput(parameters=parameters)

    @property
    def outputs(self) -> Transput:
        """
        Parses the input parameters and artifacts from the workflow step and
        returns them as a Transput object (which has a `parameters` and an
        `artifacts` attribute for easy access.
        """
        return Transput()


def transpile(path_to_script: Union[str, Path]) -> None:
    """
    Transpiles the `source` (a Python script using the scargo library) to Argo
    YAML via conversion to the Python Abstract Syntax Tree (AST).
    """

    # make sure that the Path is a pathlib object
    path_to_script = Path(path_to_script)

    # transpile the workflow parameters from Python to YAML
    ScargoTranspiler().transpile(path_to_script)

    with open(path_to_script, "r") as source:
        tree = ast.parse(source.read())

    astpretty.pprint(tree, show_offsets=False)
