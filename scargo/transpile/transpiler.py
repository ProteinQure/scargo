"""
Core functionality of the Python -> Argo YAML transpiler.
"""

import ast
from pathlib import Path
from typing import Any, Dict, List, Union

import astpretty

from scargo.core import WorkflowParams
from scargo.errors import ScargoTranspilerError
from scargo.transpile import entrypoint, yaml_io


def transpile_parameters(script_locals: Dict[str, Any]) -> WorkflowParams:
    """
    Retrieves the global workflow parameters from the scargo Python script
    and writes them to YAML.
    """
    workflow_param_variables = [value for value in script_locals.values() if isinstance(value, WorkflowParams)]

    if not workflow_param_variables:
        raise ScargoTranspilerError("No globally defined WorkflowParams object found.")
    elif len(workflow_param_variables) > 1:
        raise ScargoTranspilerError("Multiple global WorkflowParams objects found. Please only define one.")
    else:
        return workflow_param_variables[0]


def get_script_locals(source: str) -> Dict[str, Any]:
    """
    Execute the script (without actually running the __main__ function) in
    order to get convenient access to the locals() generated by the script.
    """

    script_locals = {}
    # globals are currently discarded, since their use in @scargo functions is not yet defined
    exec(source, {}, script_locals)

    return script_locals


def build_template(hyphenated_script_name, workflow_params, entrypoint_name: str, workflow_steps) -> Dict[str, Any]:
    """
    Convert the script to AST, traverse the tree and transpile the Python
    statements to an Argo workflow.
    """
    # initialize the transpiled workflow dictionary
    # with the typical Argo header
    transpiled_workflow = {
        "apiVersion": "argoproj.io/v1alpha1",
        "kind": "Workflow",
        "metadata": {"generateName": f"scargo-{hyphenated_script_name}-"},  # will be defined in self.transpile()
        "spec": {
            "entrypoint": entrypoint_name,
            "volumes": [{"name": "workdir", "emptyDir": {}}],
        },
    }

    transpiled_workflow["spec"]["arguments"] = {"parameters": [{"name": name} for name in workflow_params]}

    # add entrypoint template
    templates = []
    entrypoint_template = {
        "name": entrypoint_name,
        "steps": [
            [
                {
                    "name": step.hyphenated_name,
                    "template": step.template_name,
                    "arguments": {
                        "parameters": [
                            {
                                "name": name,
                                "value": value,
                            }
                            for name, value in step.inputs.parameters.items()
                        ]
                    },
                }
                for step in workflow_steps
            ]
        ],
    }
    templates.append(entrypoint_template)

    # add templates for the individual workflow steps
    for step in workflow_steps:
        templates.append(step.template)

    # update the transpiled workflow and write to YAML
    transpiled_workflow["spec"]["templates"] = templates
    return transpiled_workflow


def get_decorator_names(decorator_list: List) -> List[str]:
    decorator_names = []
    for decor in decorator_list:

        if isinstance(decor, ast.Call):
            decor_func = decor.func
            if isinstance(decor_func, ast.Name):
                decorator_names.append(decor_func.id)

        elif isinstance(decor, ast.Name):
            decorator_names.append(decor.id)

    return decorator_names


def find_entrypoint(tree) -> ast.FunctionDef:
    entrypoints = []
    for top_level_node in tree.body:
        if isinstance(top_level_node, ast.FunctionDef):

            print(astpretty.pprint(top_level_node.decorator_list[0], show_offsets=False))
            if "entrypoint" in get_decorator_names(top_level_node.decorator_list):
                entrypoints.append(top_level_node)

    if len(entrypoints) == 0:
        raise ScargoTranspilerError("no entrypoint!")
    elif len(entrypoints) > 1:
        raise ScargoTranspilerError("too many entrypoint")
    else:
        return entrypoints[0]


def transpile(path_to_script: Union[str, Path]) -> None:
    """
    Transpiles the `source` (a Python script using the scargo library) to Argo
    YAML via conversion to the Python Abstract Syntax Tree (AST).

    Performs transpilation by:
     1. Transpiling the WorkFlow Parameters
     2. Transpiling the EntryPoint function
    """

    path_to_script = Path(path_to_script)
    hyphenated_script_name = path_to_script.stem.replace("_", "-")

    with open(path_to_script, "r") as fi:
        source = fi.read()

    tree = ast.parse(source)

    # transpile the parameters and write them to a separate YAML file
    script_locals = get_script_locals(source)
    workflow_params = transpile_parameters(script_locals)
    yaml_io.write_params_to_yaml(path_to_script, workflow_params)

    entrypoint_func = find_entrypoint(tree)
    entrypoint_transpiler = entrypoint.EntrypointTranspiler(script_locals, tree)
    entrypoint_transpiler.visit(entrypoint_func)

    # create the Argo workflow YAML
    transpiled_workflow = build_template(
        hyphenated_script_name,
        workflow_params,
        entrypoint_name=entrypoint_func.name,
        workflow_steps=entrypoint_transpiler.steps,
    )
    yaml_io.write_workflow_to_yaml(path_to_script, transpiled_workflow)

    astpretty.pprint(tree, show_offsets=False)
