"""
Core functionality of the Python -> Argo YAML transpiler.
"""

import ast
from pathlib import Path
from typing import Dict, Union
import yaml

import astpretty

from scargo.core import MountPoints, WorkflowParams
from scargo.errors import ScargoTranspilerError
from scargo.transpile.utils import hyphenate, Transput


class ScargoTranspiler(ast.NodeVisitor):
    """
    Extracts and transpiles the scargo Python script to an Argo YAML workflow
    file.
    """

    def __init__(self):
        """
        Configures the Argo headers.
        """

        # initialize the transpiled workflow dictionary
        # with the typical Argo header
        self.transpiled_workflow = {
            "apiVersion": "argoproj.io/v1alpha1",
            "kind": "Workflow",
            "metadata": {"generateName": None},  # will be defined in self.transpile()
            "spec": {
                "volumes": {"name": "workdir", "emptyDir": {}},
            },
        }

    @staticmethod
    def _get_script_locals(path_to_script: Path) -> Dict:
        """
        Execute the script (without actually running the __main__ function) in
        order to get convenient access to the locals() generated by the script.
        """

        script_locals = {}
        with open(path_to_script, "r") as script:
            exec(script.read(), {}, script_locals)  # no need to keep the globals, everything we need is in the locals

        return script_locals

    def transpile_parameters(self, path_to_script: Path) -> None:
        """
        Retrieves the global workflow parameters from the scargo Python script
        and writes them to YAML.
        """
        script_locals = self._get_script_locals(path_to_script)
        workflow_param_variables = [value for value in script_locals.values() if isinstance(value, WorkflowParams)]

        if not workflow_param_variables:
            raise ScargoTranspilerError("No globally defined WorkflowParams object found.")
        elif len(workflow_param_variables) > 1:
            raise ScargoTranspilerError("Multiple global WorkflowParams objects found. Please only define one.")

        self._write_params_to_yaml(path_to_script, workflow_param_variables[0])
        return workflow_param_variables[0]

    def transpile(self, path_to_script: Path) -> None:
        """
        Convert the script to AST, traverse the tree and transpile the Python
        statements to an Argo workflow.
        """
        self.script_locals = self._get_script_locals(path_to_script)
        workflow_params = self.transpile_parameters(path_to_script)

        # set the Argo workflow name based on the script name
        hyphenated_script_name = path_to_script.stem.replace("_", "-")
        self.transpiled_workflow["metadata"]["generateName"] = f"scargo-{hyphenated_script_name}-"

        # transpile the parameters and write them to a separate YAML
        # as well as include their names in the main YAML workflow file
        self.transpiled_workflow["arguments"] = {"parameters": [{"name": name} for name in workflow_params]}

        # parse the AST tree
        with open(path_to_script, "r") as source:
            self.tree = ast.parse(source.read())
        self.visit(self.tree)  # traverse the tree

        # add entrypoint
        self.transpiled_workflow["entrypoint"] = self.entrypoint

        # TODO add step templates
        templates = []
        entrypoint_template = {
            "name": self.entrypoint,
            "steps": [
                {
                    "name": hyphenate(step.name),
                    "template": f"{hyphenate(step.name)}-template",
                    "arguments": {
                        "parameters": [
                            {
                                "name": name,
                                "value": value,
                            }
                            for name, value in step.inputs.parameters.items()
                        ]
                    },
                }
                for step in self.steps
            ],
        }
        templates.append(entrypoint_template)

        # TODO: add templates
        self.transpiled_workflow["templates"] = templates

        self._write_workflow_to_yaml(path_to_script, self.transpiled_workflow)

    def visit_FunctionDef(self, node: ast.FunctionDef) -> None:
        """
        Visits all FunctionDef nodes and retrieves:
            - the name of the function with the @entrypoint decorator
        """

        if isinstance(node.decorator_list[0], ast.Name):
            if node.decorator_list[0].id == "entrypoint":
                self.entrypoint = node.name

                self.steps = []
                for expression in node.body:
                    if isinstance(expression.value, ast.Call):
                        self.steps.append(
                            WorkflowStep(call_node=expression.value, locals_context=self.script_locals, tree=self.tree)
                        )
                        pass

    @staticmethod
    def _write_workflow_to_yaml(path_to_script: Path, transpiled_workflow: Dict) -> None:
        """
        Writes the `transpiled_workflow` to a YAML file in the same directory as the
        original Python input script.
        """

        filename = f"{path_to_script.stem.replace('_', '-')}.yaml"
        with open(path_to_script.parent / filename, "w+") as yaml_out:
            yaml.dump(transpiled_workflow, yaml_out, sort_keys=False)

    @staticmethod
    def _write_params_to_yaml(path_to_script: Path, parameters: Dict) -> None:
        """
        Writes the `parameters` to a YAML file in the same directory as the
        original Python input script.
        """

        filename = f"{path_to_script.stem.replace('_', '-')}-parameters.yaml"
        with open(path_to_script.parent / filename, "w+") as yaml_out:
            yaml.dump(dict(parameters), yaml_out)


class WorkflowStep:
    """
    A single step/template in the workflow.
    """

    def __init__(self, call_node: ast.Call, locals_context: Dict, tree: ast.Module) -> None:
        """
        Create a new WorkflowStep.

        Parameters
        ----------
        call_node : ast.Call
            The Call node that invokes this workflow step. In other words the
            Call node which calls the @scargo decorated function in the
            @entrypoint function.
        locals_context : Dict
            The locals() context resulting from `exec`uting the Python scargo
            script. Should contain all the global imports, variables and
            function definitions.
        tree : ast.Module
            The entire (!) Abstract Syntax Tree of the Python scargo script.
        """
        self.call_node = call_node
        self.locals_context = locals_context
        self.tree = tree

    @property
    def name(self) -> str:
        """
        Returns the name of the workflow step.
        """
        return self.call_node.func.id

    @staticmethod
    def _get_variable_from_args_or_kwargs(
        node: ast.Call, variable_name: str, arg_index: int
    ) -> Union[ast.keyword, ast.Dict]:
        """
        We don't know if the user has passed a positional or a keyworded
        argument to `node` which result in different ASTs. Since this is a
        common occurence, this method figures it out for you and returns the
        node representing the variable.
        """
        if node.args and len(node.args) > arg_index:
            return node.args[arg_index]
        elif node.keywords:
            filtered_keywords = list(filter(lambda k: k.arg == variable_name, node.keywords))
            if filtered_keywords:
                return filtered_keywords[0].value
        else:
            raise ScargoTranspilerError(f"Can't parse {variable_name} from {node.func.id}.")

    def _resolve_workflow_param(self, node: ast.Subscript) -> str:
        """
        Given a Subscript or Constant node (`raw_parameter`) this method uses the
        locals_context of the scargo script and the AST to transpile this node
        either into the actual parameter value or into a reference to the
        global Argo workflow parameters.
        """

        value = None
        if isinstance(node, ast.Subscript):
            # could be a list, tuple or dict
            subscripted_object = node.value.id
            if subscripted_object in self.locals_context:
                if isinstance(self.locals_context[subscripted_object], WorkflowParams):
                    value = "{{" + f"workflow.parameters.{node.slice.value.value}" + "}}"

        if value is None:
            raise ScargoTranspilerError(f"Cannot resolve parameter value from node type {type(node)}")

        return value

    def _is_workflow_param(self, object_name: str) -> bool:
        """
        Checks if the `object_name` is a global WorkflowParams object.
        """
        if object_name in self.locals_context:
            if isinstance(self.locals_context[object_name], WorkflowParams):
                return True
        return False

    def _is_mount_points(self, object_name: str) -> bool:
        """
        Checks if the `object_name` is a global MountPoints object.
        """
        if object_name in self.locals_context:
            if isinstance(self.locals_context[object_name], MountPoints):
                return True
        return False

    def _resolve_mount_points(self, node: ast.Subscript) -> str:
        """
        Resolves a `node` if the object it refers to is a `MountPoints`
        instance. This requires a few extra steps since `MountPoints` tend to
        be nested objects containing one or several `MountPoint` (singular!)
        objects which in turn can be made up of references to the global
        workflow parameter object.
        """
        subscripted_object_name = node.value.id
        subscript = node.slice.value.value

        # use the AST to check if this was defined as a string
        for toplevel_node in ast.iter_child_nodes(self.tree):
            if isinstance(toplevel_node, ast.Assign):
                relevant_target = list(filter(lambda t: t.id == subscripted_object_name, toplevel_node.targets))
                if relevant_target and relevant_target[0].id == subscripted_object_name:

                    subscripted_object = self._get_variable_from_args_or_kwargs(toplevel_node.value, "__dict", 0)
                    resolved_node = list(
                        filter(
                            lambda tuple_: tuple_[0].value == subscript,
                            zip(subscripted_object.keys, subscripted_object.values),
                        )
                    )[0][1]

                    remote_subscript_object = self._get_variable_from_args_or_kwargs(resolved_node, "remote", 1)
                    if self._is_workflow_param(remote_subscript_object.value.id):
                        return self._resolve_workflow_param(remote_subscript_object)

        raise ScargoTranspilerError(f"Cannot resolve mount point {subscripted_object_name}[{subscript}]")

    def _resolve_subscript(self, node: ast.Subscript):
        """
        General method that is used to resolve Subscript nodes which typically
        tend to involve the global workflow parameters or mount points.
        """
        subscripted_object_name = node.value.id
        subscript = node.slice.value.value

        if self._is_workflow_param(subscripted_object_name):
            return self._resolve_workflow_param(node)
        elif self._is_mount_points(subscripted_object_name):
            return self._resolve_mount_points(node)
        else:
            raise ScargoTranspilerError(f"Cannot resolve {subscripted_object_name}[{subscript}].")

    def _transpile_artifact(self, raw_artifact: Union[ast.Constant, ast.Call], output: bool) -> str:
        """
        Given a Call and Constant node (`raw_artifact`) this method uses the
        locals_context of the scargo script and the AST to transpile this node
        either into the actual artifact value or into a reference to the global
        Argo workflow parameters.
        """

        artifact = None
        if isinstance(raw_artifact, ast.Call) and raw_artifact.func.id == "FileOutput":

            root = self._resolve_subscript(self._get_variable_from_args_or_kwargs(raw_artifact, "root", 0))
            path = self._resolve_subscript(self._get_variable_from_args_or_kwargs(raw_artifact, "path", 1))

            artifact = {
                "path": "/workdir/out" if output else "/workdir/in",
                "archive": {
                    "none": {},
                },
                "s3": {
                    "endpoint": "s3.amazonaws.com",
                    "bucket": root,
                    "key": path,
                },
            }

        if artifact is None:
            raise ScargoTranspilerError(
                f"Cannot resolve artifact {raw_artifact.func.id} from node type {type(raw_artifact)}"
            )

        return artifact

    @property
    def inputs(self) -> Transput:
        """
        Parses the input parameters and artifacts from the workflow step and
        returns them as a Transput object (which has a `parameters` and an
        `artifacts` attribute for easy access.
        """
        scargo_inputs = self._get_variable_from_args_or_kwargs(self.call_node, "scargo_in", 0)
        if scargo_inputs.func.id != "ScargoInput":
            raise ScargoTranspilerError("First argument to a @scargo function must be a `ScargoInput`.")

        raw_parameters = self._get_variable_from_args_or_kwargs(scargo_inputs, "parameters", 0)
        parameters = {}
        if raw_parameters:
            for key, value in zip(raw_parameters.keys, raw_parameters.values):
                parameters[key.value] = self._resolve_workflow_param(value)

        raw_artifacts = self._get_variable_from_args_or_kwargs(scargo_inputs, "artifacts", 1)
        artifacts = {}
        if raw_artifacts:
            for key, raw_artifact in zip(raw_artifacts.keys, raw_artifacts.values):
                artifacts[key.value] = self._transpile_artifact(raw_artifact, output=False)

        return Transput(parameters=parameters, artifacts=artifacts)

    @property
    def outputs(self) -> Transput:
        """
        Parses the input parameters and artifacts from the workflow step and
        returns them as a Transput object (which has a `parameters` and an
        `artifacts` attribute for easy access.
        """
        scargo_outputs = self._get_variable_from_args_or_kwargs(self.call_node, "scargo_out", 1)
        if scargo_outputs.func.id != "ScargoOutput":
            raise ScargoTranspilerError("Second argument to a @scargo function must be a `ScargoOutput`.")

        raw_parameters = self._get_variable_from_args_or_kwargs(scargo_outputs, "parameters", 0)
        parameters = {}
        if raw_parameters:
            for key, value in zip(raw_parameters.keys, raw_parameters.values):
                parameters[key.value] = self._resolve_workflow_param(value)

        raw_artifacts = self._get_variable_from_args_or_kwargs(scargo_outputs, "artifacts", 1)
        artifacts = {}
        if raw_artifacts:
            for key, raw_artifact in zip(raw_artifacts.keys, raw_artifacts.values):
                artifacts[key.value] = self._transpile_artifact(raw_artifact, output=True)

        return Transput(parameters=parameters, artifacts=artifacts)


def transpile(path_to_script: Union[str, Path]) -> None:
    """
    Transpiles the `source` (a Python script using the scargo library) to Argo
    YAML via conversion to the Python Abstract Syntax Tree (AST).
    """

    # make sure that the Path is a pathlib object
    path_to_script = Path(path_to_script)

    # transpile the workflow parameters from Python to YAML
    ScargoTranspiler().transpile(path_to_script)

    with open(path_to_script, "r") as source:
        tree = ast.parse(source.read())

    astpretty.pprint(tree, show_offsets=False)
