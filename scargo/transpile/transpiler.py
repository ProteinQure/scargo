"""
Core functionality of the Python -> Argo YAML transpiler.
"""

import ast
from pathlib import Path
from typing import Dict, Union
import yaml

import astpretty

from scargo.core import WorkflowParams
from scargo.errors import ScargoTranspilerError
from scargo.transpile.utils import hyphenate


class ScargoTranspiler(ast.NodeVisitor):
    """
    Extracts and transpiles the scargo Python script to an Argo YAML workflow
    file.
    """

    def __init__(self):
        """
        Configures the Argo headers.
        """

        # initialize the transpiled workflow dictionary
        # with the typical Argo header
        self.transpiled_workflow = {
            "apiVersion": "argoproj.io/v1alpha1",
            "kind": "Workflow",
            "metadata": {"generateName": None},  # will be defined in self.transpile()
            "spec": {
                "volumes": {"name": "workdir", "emptyDir": {}},
            },
        }

    @staticmethod
    def _get_script_locals(path_to_script: Path) -> Dict:
        """
        Execute the script (without actually running the __main__ function) in
        order to get convenient access to the locals() generated by the script.
        """

        script_locals = {}
        with open(path_to_script, "r") as script:
            exec(script.read(), {}, script_locals)  # no need to keep the globals, everything we need is in the locals

        return script_locals

    def transpile_parameters(self, path_to_script: Path) -> None:
        """
        Retrieves the global workflow parameters from the scargo Python script
        and writes them to YAML.
        """
        script_locals = self._get_script_locals(path_to_script)
        workflow_param_variables = [value for value in script_locals.values() if isinstance(value, WorkflowParams)]

        if not workflow_param_variables:
            raise ScargoTranspilerError("No globally defined WorkflowParams object found.")
        elif len(workflow_param_variables) > 1:
            raise ScargoTranspilerError("Multiple global WorkflowParams objects found. Please only define one.")

        self._write_params_to_yaml(path_to_script, workflow_param_variables[0])
        return workflow_param_variables[0]

    def transpile(self, path_to_script: Path) -> None:
        """
        Convert the script to AST, traverse the tree and transpile the Python
        statements to an Argo workflow.
        """

        script_locals = self._get_script_locals(path_to_script)
        workflow_params = self.transpile_parameters(path_to_script)

        # set the Argo workflow name based on the script name
        hyphenated_script_name = path_to_script.stem.replace("_", "-")
        self.transpiled_workflow["metadata"]["generateName"] = f"scargo-{hyphenated_script_name}-"

        # transpile the parameters and write them to a separate YAML
        # as well as include their names in the main YAML workflow file
        self.transpiled_workflow["arguments"] = {"parameters": [{"name": name} for name in workflow_params]}

        # parse the AST tree
        with open(path_to_script, "r") as source:
            tree = ast.parse(source.read())
        self.visit(tree)  # traverse the tree

        # add entrypoint
        self.transpiled_workflow["entrypoint"] = self.entrypoint

        # TODO add step templates
        templates = []
        entrypoint_template = {
            "name": self.entrypoint,
            "steps": [
                {
                    "name": hyphenate(step["name"]),
                    "template": f"{hyphenate(step['name'])}-template",
                    "arguments": {
                        "parameters": [
                            {
                                "name": "TODO",
                                "value": "TODO",
                            }
                            for parameter in step["inputs"].keywords[0].value.keys
                        ]
                    },
                }
                for step in self.steps
            ],
        }
        templates.append(entrypoint_template)

        # TODO: add templates
        self.transpiled_workflow["templates"] = templates

        self._write_workflow_to_yaml(path_to_script, self.transpiled_workflow)

    def visit_FunctionDef(self, node: ast.FunctionDef) -> None:
        """
        Visits all FunctionDef nodes and retrieves:
            - the name of the function with the @entrypoint decorator
        """

        if isinstance(node.decorator_list[0], ast.Name):
            if node.decorator_list[0].id == "entrypoint":
                self.entrypoint = node.name

                self.steps = []
                for expression in node.body:
                    if isinstance(expression.value, ast.Call):
                        # TODO: maybe create simple Steps class that parses
                        # this into an easily accessible object?
                        self.steps.append(
                            {
                                "name": expression.value.func.id,
                                "inputs": expression.value.args[0],
                                "outputs": expression.value.args[1],
                            }
                        )

    @staticmethod
    def _write_workflow_to_yaml(path_to_script: Path, transpiled_workflow: Dict) -> None:
        """
        Writes the `transpiled_workflow` to a YAML file in the same directory as the
        original Python input script.
        """

        filename = f"{path_to_script.stem.replace('_', '-')}.yaml"
        with open(path_to_script.parent / filename, "w+") as yaml_out:
            yaml.dump(transpiled_workflow, yaml_out, sort_keys=False)

    @staticmethod
    def _write_params_to_yaml(path_to_script: Path, parameters: Dict) -> None:
        """
        Writes the `parameters` to a YAML file in the same directory as the
        original Python input script.
        """

        filename = f"{path_to_script.stem.replace('_', '-')}-parameters.yaml"
        with open(path_to_script.parent / filename, "w+") as yaml_out:
            yaml.dump(dict(parameters), yaml_out)


def transpile(path_to_script: Union[str, Path]) -> None:
    """
    Transpiles the `source` (a Python script using the scargo library) to Argo
    YAML via conversion to the Python Abstract Syntax Tree (AST).
    """

    # make sure that the Path is a pathlib object
    path_to_script = Path(path_to_script)

    # transpile the workflow parameters from Python to YAML
    ScargoTranspiler().transpile(path_to_script)

    with open(path_to_script, "r") as source:
        tree = ast.parse(source.read())

    astpretty.pprint(tree, show_offsets=False)
